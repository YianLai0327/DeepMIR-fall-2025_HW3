# train.py
import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import TransfoXLConfig, TransfoXLLMHeadModel, GPT2Config, GPT2LMHeadModel
from pathlib import Path
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
from miditok import REMI
import shutil
import subprocess

# å°å…¥ä½ çš„ dataloader
from dataloader import create_dataloaders

# def generate_music(
#     model,
#     tokenizer,
#     num_bars: int = 32,
#     temperature: float = 1.0,
#     top_k: int = 50,
#     top_p: float = 0.95,
#     device: str = "cuda",
#     seed: int = None,
#     vocab_size: int = 3000,  # æ·»åŠ  vocab_size å‚æ•°
# ):
#     """
#     ç”Ÿæˆ unconditional music sequence
#     """
#     model.eval()
    
#     if seed is not None:
#         torch.manual_seed(seed)
    
#     # ä¼°ç®—éœ€è¦çš„ token æ•°é‡
#     tokens_per_bar = 81
#     max_length = num_bars * tokens_per_bar
    
#     # ===== å…³é”®ä¿®æ­£ï¼šç¡®ä¿èµ·å§‹ token æœ‰æ•ˆ =====
#     # æ–¹æ³• 1: ä½¿ç”¨ä¸€ä¸ªå®‰å…¨çš„èµ·å§‹ token
#     # é¿å…ä½¿ç”¨ PAD (0)ï¼Œé€‰æ‹©ä¸€ä¸ªåœ¨è®­ç»ƒæ•°æ®ä¸­å¸¸è§çš„ token
    
#     # ä»è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¸­é€‰æ‹©æœ€å¸¸è§çš„èµ·å§‹ token
#     # é€šå¸¸ REMI çš„åºåˆ—ä»¥ Bar æˆ– Position token å¼€å§‹
#     # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨ä¸€ä¸ªä¸­é—´å€¼ï¼Œé¿å…è¾¹ç•Œæƒ…å†µ
#     start_token_id = 1  # æˆ–è€…å¯ä»¥æ˜¯ tokenizer çš„ BOS token
    
#     # ç¡®ä¿èµ·å§‹ token åœ¨æœ‰æ•ˆèŒƒå›´å†…
#     start_token_id = max(1, min(start_token_id, vocab_size - 1))
    
#     generated_ids = torch.tensor([[start_token_id]], dtype=torch.long, device=device)
    
#     print(f"  Starting generation from token {start_token_id}")
#     print(f"  Target length: ~{max_length} tokens ({num_bars} bars)")
#     print(f"  Vocab size: {vocab_size}")
    
#     # é€æ­¥ç”Ÿæˆ
#     with torch.no_grad():
#         for step in range(max_length - 1):
#             # Forward pass
#             outputs = model(input_ids=generated_ids)
#             next_token_logits = outputs.logits[:, -1, :].clone()  # (1, vocab_size)
            
#             # ===== å…³é”®ä¿®æ­£ï¼šé™åˆ¶ logits åˆ°æœ‰æ•ˆçš„ vocab èŒƒå›´ =====
#             # å°†è¶…å‡º vocab_size çš„ logits è®¾ä¸º -inf
#             if next_token_logits.shape[-1] > vocab_size:
#                 next_token_logits[:, vocab_size:] = float('-inf')
            
#             # åº”ç”¨æ¸©åº¦
#             next_token_logits = next_token_logits / temperature
            
#             # Top-k è¿‡æ»¤
#             if top_k > 0:
#                 top_k_actual = min(top_k, next_token_logits.shape[-1])
#                 indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k_actual)[0][..., -1, None]
#                 next_token_logits[indices_to_remove] = float('-inf')
            
#             # Top-p (nucleus) è¿‡æ»¤
#             if top_p < 1.0:
#                 sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)
#                 cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                
#                 sorted_indices_to_remove = cumulative_probs > top_p
#                 sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
#                 sorted_indices_to_remove[..., 0] = 0
                
#                 indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
#                 next_token_logits[indices_to_remove] = float('-inf')
            
#             # ===== å…³é”®ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ tokens =====
#             # å¦‚æœæ‰€æœ‰ logits éƒ½æ˜¯ -infï¼Œå›é€€åˆ°å‡åŒ€åˆ†å¸ƒ
#             if torch.all(torch.isinf(next_token_logits)):
#                 print(f"  Warning: All logits filtered at step {step}, using uniform distribution")
#                 next_token_logits = torch.zeros_like(next_token_logits)
            
#             # é‡‡æ ·
#             probs = torch.softmax(next_token_logits, dim=-1)
            
#             # ===== å…³é”®ä¿®æ­£ï¼šç¡®ä¿æ¦‚ç‡æœ‰æ•ˆ =====
#             if torch.any(torch.isnan(probs)) or torch.all(probs == 0):
#                 print(f"  Warning: Invalid probabilities at step {step}, using uniform")
#                 probs = torch.ones_like(probs) / probs.shape[-1]
            
#             next_token = torch.multinomial(probs, num_samples=1)
            
#             # ===== å…³é”®ä¿®æ­£ï¼šéªŒè¯ç”Ÿæˆçš„ token =====
#             next_token_value = next_token.item()
#             if next_token_value < 0 or next_token_value >= vocab_size:
#                 print(f"  Warning: Generated invalid token {next_token_value}, clipping to valid range")
#                 next_token_value = max(1, min(next_token_value, vocab_size - 1))
#                 next_token = torch.tensor([[next_token_value]], dtype=torch.long, device=device)
            
#             # æ·»åŠ åˆ°åºåˆ—
#             generated_ids = torch.cat([generated_ids, next_token], dim=-1)
            
#             # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯ 100 stepsï¼‰
#             if (step + 1) % 100 == 0:
#                 print(f"  Generated {step + 1}/{max_length} tokens...")
            
#             # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡é•¿åº¦
#             if generated_ids.shape[1] >= max_length:
#                 break
    
#     print(f"  âœ“ Generation completed: {generated_ids.shape[1]} tokens")
#     return generated_ids[0].cpu().tolist()

def generate_music(
    model,
    tokenizer,
    num_bars: int = 32,
    temperature: float = 1.0,
    top_k: int = 50,
    top_p: float = 0.95,
    device: str = "cuda",
    seed: int = None,
    vocab_size: int = 3000,
):
    """
    ç”Ÿæˆ unconditional music sequence
    - ç¸½é•·åº¦å°é½Šã€Œnum_barsã€ï¼Œä½†æ¯æ¬¡ä¸Ÿé€² model çš„ context é•·åº¦ <= model.config.n_positions
    """
    model.eval()
    
    if seed is not None:
        torch.manual_seed(seed)
        if device == "cuda":
            torch.cuda.manual_seed_all(seed)
    
    # ç²—ä¼°æ¯å°ç¯€ token æ•¸
    tokens_per_bar = 81              # ä½ åŸæœ¬çš„ä¼°è¨ˆï¼Œä¹‹å¾Œå¯ä»¥æ ¹æ“š dataset å¹³å‡å†ä¿®
    max_length = num_bars * tokens_per_bar

    # å–å¾— model èƒ½åƒçš„æœ€å¤§ context é•·åº¦ï¼ˆGPT2 ç”¨ n_positionsï¼‰
    max_context = getattr(model.config, "n_positions", None)
    if max_context is None:
        max_context = getattr(model.config, "max_position_embeddings", 512)
    if max_context is None:
        max_context = 512  # fallback

    print(f"  Target bars: {num_bars}")
    print(f"  Approx target length (tokens): {max_length}")
    print(f"  Model max context length: {max_context}")
    print(f"  Vocab size: {vocab_size}")

    # èµ·å§‹ tokenï¼ˆä¸è¦ç”¨ 0 æ¯”è¼ƒå®‰å…¨ï¼‰
    start_token_id = 1
    start_token_id = max(1, min(start_token_id, vocab_size - 1))
    
    generated_ids = torch.tensor([[start_token_id]], dtype=torch.long, device=device)
    print(f"  Starting generation from token {start_token_id}")
    
    with torch.no_grad():
        for step in range(max_length - 1):
            # ğŸ”¥ sliding windowï¼šåªæ‹¿æœ€å¾Œ max_context å€‹ token ä¸Ÿé€² model
            input_ids = generated_ids[:, -max_context:]
            
            outputs = model(input_ids=input_ids)
            next_token_logits = outputs.logits[:, -1, :].clone()  # (1, vocab)

            # ç¢ºä¿ä¸æœƒé¸åˆ°è¶…å‡º vocab çš„ id
            if next_token_logits.shape[-1] > vocab_size:
                next_token_logits[:, vocab_size:] = float('-inf')
            
            # æº«åº¦
            next_token_logits = next_token_logits / temperature
            
            # Top-k
            if top_k > 0:
                top_k_actual = min(top_k, next_token_logits.shape[-1])
                kth_vals = torch.topk(next_token_logits, top_k_actual)[0][..., -1, None]
                indices_to_remove = next_token_logits < kth_vals
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus)
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)
                sorted_probs = torch.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_token_logits[indices_to_remove] = float('-inf')
            
            # è‹¥å…¨è¢«éæ¿¾æ‰ï¼Œå›é€€æˆ uniform
            if torch.all(torch.isinf(next_token_logits)):
                print(f"  Warning: All logits filtered at step {step}, using uniform distribution")
                next_token_logits = torch.zeros_like(next_token_logits)
            
            probs = torch.softmax(next_token_logits, dim=-1)
            if torch.any(torch.isnan(probs)) or torch.all(probs == 0):
                print(f"  Warning: Invalid probabilities at step {step}, using uniform")
                probs = torch.ones_like(probs) / probs.shape[-1]
            
            next_token = torch.multinomial(probs, num_samples=1)

            # ä¿éšªï¼šæŠŠ token id å£“å›åˆæ³•ç¯„åœ
            next_token_value = next_token.item()
            if next_token_value < 0 or next_token_value >= vocab_size:
                print(f"  Warning: Generated invalid token {next_token_value}, clipping to valid range")
                next_token_value = max(1, min(next_token_value, vocab_size - 1))
                next_token = torch.tensor([[next_token_value]], dtype=torch.long, device=device)
            
            generated_ids = torch.cat([generated_ids, next_token], dim=-1)
            
            if (step + 1) % 100 == 0:
                print(f"  Generated {step + 1}/{max_length - 1} tokens...")
            
            if generated_ids.shape[1] >= max_length:
                break
    
    print(f"  âœ“ Generation completed: {generated_ids.shape[1]} tokens "
          f"(approx {generated_ids.shape[1]/tokens_per_bar:.1f} bars)")
    return generated_ids[0].cpu().tolist()

def save_generated_midi(
    token_ids,
    tokenizer: REMI,
    output_path: str,
):
    """
    å°†ç”Ÿæˆçš„ tokens è½¬æ¢ä¸º MIDI å¹¶ä¿å­˜
    """
    try:
        # ===== å…³é”®ä¿®æ­£ï¼šæ­£ç¡®ä½¿ç”¨ REMI tokenizer =====
        # REMI çš„ decode æ–¹æ³•éœ€è¦ token IDs
        
        # æ–¹æ³• 1: ä½¿ç”¨ ids_to_tokens + tokens_to_midi
        # tokens = tokenizer.ids_to_tokens(token_ids)
        generated_midi = tokenizer.decode([token_ids])
        # generated_midi = (token_ids)
        
        # ä¿å­˜ MIDI
        generated_midi.dump_midi(output_path)
        print(f"  âœ“ Saved MIDI to: {output_path}")
        return True
        
    except Exception as e:
        print(f"  âœ— Failed to save MIDI: {e}")
        print(f"  Token IDs range: [{min(token_ids)}, {max(token_ids)}]")
        print(f"  Number of tokens: {len(token_ids)}")
        
        # å°è¯•ä¿å­˜ token IDs ä»¥ä¾›è°ƒè¯•
        try:
            import json
            debug_path = output_path.replace('.mid', '_tokens.json')
            with open(debug_path, 'w') as f:
                json.dump({
                    'token_ids': token_ids,  # åªä¿å­˜å‰ 100 ä¸ª
                    'total_tokens': len(token_ids),
                    'min_id': min(token_ids),
                    'max_id': max(token_ids),
                }, f, indent=2)
            print(f"  âœ“ Saved debug tokens to: {debug_path}")
        except:
            pass
        
        return False


def run_inference_test(
    model,
    tokenizer: REMI,
    epoch: int,
    output_dir: str,
    vocab_size: int,  # æ·»åŠ  vocab_size å‚æ•°
    device: str = "cuda",
):
    """
    è¿è¡Œ inference æµ‹è¯•
    """
    print(f"\n{'='*60}")
    print(f"Running inference test at epoch {epoch}")
    print(f"{'='*60}")
    
    epoch_dir = Path(output_dir) / f"epoch_{epoch}"
    epoch_dir.mkdir(parents=True, exist_ok=True)
    
    # æµ‹è¯•ä¸åŒçš„é‡‡æ ·é…ç½®
    configs = [
        {"name": "greedy", "temperature": 0.8, "top_k": 50, "top_p": 0.95},
        # {"name": "diverse", "temperature": 1.2, "top_k": 100, "top_p": 0.9},
        {"name": "conservative", "temperature": 0.6, "top_k": 30, "top_p": 0.95},
    ]
    
    success_count = 0
    
    for config in configs:
        midi_saved = False
        print(f"\n{'='*50}")
        print(f"Config: {config['name']}")
        print(f"  Temperature: {config['temperature']}")
        print(f"  Top-k: {config['top_k']}")
        print(f"  Top-p: {config['top_p']}")
        print(f"{'='*50}")
        
        try:
            # ç”Ÿæˆ
            generated_ids = generate_music(
                model=model,
                tokenizer=tokenizer,
                num_bars=32,
                temperature=config['temperature'],
                top_k=config['top_k'],
                top_p=config['top_p'],
                device=device,
                seed=42 + configs.index(config),  # ä¸åŒçš„ç§å­
                vocab_size=vocab_size,  # ä¼ é€’ vocab_size
            )
            
            print(f"\n  Generated token statistics:")
            print(f"    Total: {len(generated_ids)}")
            print(f"    Range: [{min(generated_ids)}, {max(generated_ids)}]")
            print(f"    First 30: {generated_ids[:30]}")
            
            # ä¿å­˜ MIDI
            output_path = epoch_dir / f"{config['name']}.mid"
            if save_generated_midi(generated_ids, tokenizer, str(output_path)):
                success_count += 1
                midi_saved = True
                
        except Exception as e:
            print(f"\n  âœ— Generation failed: {e}")
            import traceback
            traceback.print_exc()

        if midi_saved and output_path.exists():
            # covert midi to wav using fluidsynth
            try:
                wav_output_path = epoch_dir / f"{config['name']}.wav"
                midi_to_wav_with_fluidsynth(
                    midi_path=str(output_path),
                    wav_path=str(wav_output_path),
                    sound_font="/usr/share/sounds/sf2/FluidR3_GM.sf2",
                    sample_rate=44100,
                )
            except Exception as e:
                print(f"\n  âœ— WAV conversion failed: {e}")
                import traceback
                traceback.print_exc()

        else:
            print(f"\n  âœ— MIDI file not found, skipping WAV conversion.")

        # show progress
        print(f"  âœ“ Finished generation for config: {config['name']}")

        
    print(f"\n{'='*60}")
    print(f"âœ“ Inference test completed: {success_count}/{len(configs)} successful")
    print(f"  Files saved to: {epoch_dir}")
    print(f"{'='*60}")

def midi_to_wav_with_fluidsynth(
    midi_path: str,
    wav_path: str,
    sound_font: str,
    sample_rate: int = 44100,
):
    """
    ä½¿ç”¨ Fluidsynth å°‡ MIDI è½‰æˆ WAVã€‚

    éœ€è¦ï¼š
    - ç³»çµ±å·²å®‰è£ `fluidsynth` æŒ‡ä»¤
    - æœ‰å¯ç”¨çš„ .sf2 soundfont æª”æ¡ˆ
    """
    if shutil.which("fluidsynth") is None:
        raise RuntimeError(
            "fluidsynth not found in PATHï¼Œè«‹å…ˆå®‰è£ Fluidsynthï¼Œ"
            "ä¾‹å¦‚: sudo apt-get install fluidsynth"
        )

    midi_path = str(midi_path)
    wav_path = str(wav_path)

    cmd = [
        "fluidsynth",
        "-ni", sound_font,   # soundfont
        midi_path,           # midi input
        "-F", wav_path,      # output wav file
        "-r", str(sample_rate),
    ]

    print(f"  â†’ Converting MIDI to WAV with Fluidsynth:")
    print(f"    Command: {' '.join(cmd)}")
    subprocess.run(cmd, check=True)
    print(f"  âœ“ WAV saved to: {wav_path}")

def train_model(
    corpus_path: str = "tokenizer/remi_tokenized_corpus.pkl",
    tokenizer_path: str = "tokenizer/remi_tokenizer.json",
    batch_size: int = 32,  # æ ¹æ“šä½ çš„ GPU èª¿æ•´
    seq_len: int = 512,
    num_epochs: int = 100,
    learning_rate: float = 2.5e-4,
    device: str = "cuda",
    checkpoint_dir: str = "checkpoints",
):
    # å»ºç«‹ checkpoint ç›®éŒ„
    Path(checkpoint_dir).mkdir(exist_ok=True)
    
    # ===== 1. è¼‰å…¥è³‡æ–™ =====
    print("="*60)
    print("Loading data...")
    print("="*60)
    
    train_loader, vocab_size = create_dataloaders(
        corpus_path=corpus_path,
        tokenizer_path=tokenizer_path,
        batch_size=batch_size,
        seq_len=seq_len,
        overlap=64,
        # num_workers=4,  # ç¢ºèªå¯ä»¥ç”¨å¾Œå†èª¿æ•´
        num_workers=12,     # CPU æ ¸å¿ƒå¤šå°±é–‹å¤§
        # train_ratio=0.95,
    )

    tokenizer = REMI(params=tokenizer_path)
    
    # ===== 2. å»ºç«‹æ¨¡å‹ =====
    print("\n" + "="*60)
    print("Building model...")
    print("="*60)

    config = GPT2Config(
        vocab_size=vocab_size,
        n_positions=seq_len,      # æœ€å¤§åºåˆ—é•·åº¦
        n_embd=512,               # embedding dimension
        n_layer=12,               # number of layers
        n_head=8,                 # attention heads
        n_inner=2048,             # FFN dimension
        resid_pdrop=0.1,
        embd_pdrop=0.1,
        attn_pdrop=0.1,
    )

    model = GPT2LMHeadModel(config).to(device)
    # config = TransfoXLConfig()
    # model = TransfoXLLMHeadModel(config).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"Model has {total_params} total parameters, "
            f"{trainable_params} trainable parameters.")
    
    print(f"Model Configuration:")
    print(config)
    
    # ===== 3. å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨ =====
    optimizer = AdamW(
        model.parameters(),
        lr=learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01,
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs,
        eta_min=1e-5,
    )
    
    # ===== 4. è¨“ç·´æ­·å²è¨˜éŒ„ =====
    history = {
        'train_loss': [],
        # 'val_loss': [],
        'learning_rate': [],
    }
    
    best_val_loss = float('inf')

    LOSS_THRESHOLDS = [6.0, 4.0, 3.0, 2.5, 2.0, 1.5, 1.2, 1.0]
    generated_at_loss = set()
    
    # ===== 5. è¨“ç·´å¾ªç’° =====
    print("\n" + "="*60)
    print("Starting training...")
    print("="*60)
    
    for epoch in range(num_epochs):
        # --- Training ---
        model.train()
        train_loss = 0.0
        train_steps = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, (input_ids, target_ids) in enumerate(progress_bar):
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)
            
            # Forward pass
            outputs = model(
                input_ids=input_ids,
                labels=target_ids,
            )
            loss = outputs.loss
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_steps += 1
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{train_loss/train_steps:.4f}'
            })
        
        avg_train_loss = train_loss / train_steps
        
        # # --- Validation ---
        # model.eval()
        # val_loss = 0.0
        # val_steps = 0
        
        # with torch.no_grad():
        #     for input_ids, target_ids in val_loader:
        #         input_ids = input_ids.to(device)
        #         target_ids = target_ids.to(device)
                
        #         outputs = model(
        #             input_ids=input_ids,
        #             labels=target_ids,
        #         )
        #         val_loss += outputs.loss.item()
        #         val_steps += 1
        
        # avg_val_loss = val_loss / val_steps
        
        # æ›´æ–°å­¸ç¿’ç‡
        current_lr = scheduler.get_last_lr()[0]
        scheduler.step()
        
        # è¨˜éŒ„æ­·å²
        history['train_loss'].append(avg_train_loss)
        # history['val_loss'].append(avg_val_loss)
        history['learning_rate'].append(current_lr)
        
        # è¼¸å‡ºçµæœ
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{num_epochs} Summary:")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        # print(f"  Val Loss:   {avg_val_loss:.4f}")
        print(f"  LR:         {current_lr:.2e}")
        print(f"{'='*60}\n")

        if (epoch % 10 == 0) and (epoch >= 60) and (avg_train_loss <= 1.2):
        # if True:
            # inference when reaching certain loss thresholds
            print(f"âœ“ Running inference test at epoch {epoch} with train loss {avg_train_loss:.4f}")
            run_inference_test(
                model=model,
                tokenizer=tokenizer,
                epoch=epoch,
                output_dir=checkpoint_dir,
                device=device,
                vocab_size=vocab_size,
            )
            for thr in LOSS_THRESHOLDS:
                if avg_train_loss < thr:
                    generated_at_loss.add(thr)

        
        # # å„²å­˜æœ€ä½³æ¨¡å‹
        # if avg_val_loss < best_val_loss:
        #     best_val_loss = avg_val_loss
        #     torch.save({
        #         'epoch': epoch,
        #         'model_state_dict': model.state_dict(),
        #         'optimizer_state_dict': optimizer.state_dict(),
        #         'train_loss': avg_train_loss,
        #         'val_loss': avg_val_loss,
        #         'config': config.to_dict(),
        #     }, f'{checkpoint_dir}/best_model.pt')
        #     print(f"âœ“ Saved best model (val_loss: {avg_val_loss:.4f})")
        
        # å®šæœŸå„²å­˜ checkpoint
        if (epoch) % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                # 'val_loss': avg_val_loss,
                'config': config.to_dict(),
            }, f'{checkpoint_dir}/checkpoint_epoch_{epoch}_loss_{train_loss}.pt')
            print(f"âœ“ Saved checkpoint at epoch {epoch}")
                
        # å„²å­˜è¨“ç·´æ­·å²
        with open(f'{checkpoint_dir}/training_history.json', 'w') as f:
            json.dump(history, f, indent=2)
    
    # ===== 6. ç¹ªè£½è¨“ç·´æ›²ç·š =====
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    # plt.plot(history['val_loss'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Loss')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(history['learning_rate'])
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule')
    plt.grid(True)
    plt.yscale('log')
    
    plt.tight_layout()
    plt.savefig(f'{checkpoint_dir}/training_curves.png', dpi=150)
    print(f"\nâœ“ Training curves saved to {checkpoint_dir}/training_curves.png")
    
    return model, history


if __name__ == "__main__":
    # æª¢æŸ¥ CUDA
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # é–‹å§‹è¨“ç·´
    model, history = train_model(
        batch_size=32,  # æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
        seq_len=512,
        num_epochs=120,
        device=device,
    )
    
    print("\n" + "="*60)
    print("âœ“ Training completed!")
    print("="*60)